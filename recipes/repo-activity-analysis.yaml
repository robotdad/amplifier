name: "repo-activity-analysis"
description: "Analyze a GitHub repository for commits and PRs in a date range. Defaults to current repo since yesterday."
version: "1.6.0"
author: "Amplifier Recipes"
tags: ["github", "analysis", "commits", "prs", "git", "activity"]

# Repository Activity Analysis Recipe
#
# Analyzes a single GitHub repository for commits and PRs in a date range.
# Defaults to the current working directory's repo and "since yesterday".
#
# v1.4.0: Timezone handling improvements
#         - Emphasize LOCAL timezone in date parsing prompt
#         - No end date calculation for open-ended "since X" queries
#
# v1.3.0: Reliability improvements
#         - Add explicit write-files step (LLM file writes are non-deterministic)
#         - Synthesize step now returns content only, bash step writes files
#
# v1.2.0: Added _precomputed values pattern for orchestration optimization
#         - Skips date parsing LLM call when parent provides parsed values
#         - Skips URL parsing when parent provides owner/name
#         - Maintains full standalone functionality
#
# v1.1.0: Converted bash-heavy steps to type: "bash" for efficiency
#         (no LLM overhead for deterministic shell commands)
#
# Usage (defaults - analyze current repo since yesterday):
#   amplifier recipes execute repo-activity-analysis.yaml
#
# Usage (explicit repo):
#   amplifier recipes execute repo-activity-analysis.yaml --context '{
#     "repo_url": "https://github.com/microsoft/amplifier-core"
#   }'
#
# Usage (custom date range):
#   amplifier recipes execute repo-activity-analysis.yaml --context '{
#     "date_range": "last 7 days"
#   }'
#
# Requirements:
#   - gh CLI installed and authenticated
#   - Git (for detecting current repo if using defaults)

context:
  # Repository URL - leave empty to detect from current working directory
  repo_url: ""
  
  # Date range (natural language) - defaults to yesterday
  date_range: "since yesterday"
  
  # Working directory for intermediate files
  working_dir: "./ai_working"
  
  # Include deep-dive analysis for unclear changes
  include_deep_dive: true
  
  # === Optional precomputed values (for orchestration optimization) ===
  # When called from a parent recipe, these skip expensive re-computation.
  # Leave empty/default for standalone execution (recipe computes them).
  _precomputed:
    date_since_iso: ""      # e.g., "2024-12-28T00:00:00Z" - skips LLM date parsing
    date_since_date: ""     # e.g., "2024-12-28"
    date_description: ""    # e.g., "since yesterday (2024-12-28 to now)"
    repo_owner: ""          # e.g., "microsoft" - skips URL parsing
    repo_name: ""           # e.g., "amplifier-core"
  
  # Default values for conditional step outputs (prevents undefined variable errors)
  commit_analysis: {"total_analyzed": 0, "by_impact": {}, "summaries": [], "needs_deep_dive": [], "themes": [], "skipped": true}
  deep_dive_results: {"deep_dives": [], "skipped": true}
  pr_analysis: {"total_prs": 0, "merged": 0, "open": 0, "by_category": {}, "notable_prs": [], "themes": [], "skipped": true}

steps:
  # ==========================================================================
  # Step 1: Detect repo (Bash - uses precomputed or parses URL)
  # ==========================================================================
  # BASH STEP - Use precomputed owner/name if available, else parse URL
  # Optimization: Parent recipes can pass _precomputed.repo_owner/name to skip parsing
  - id: "detect-repo"
    type: "bash"
    command: |
      set -euo pipefail
      
      mkdir -p {{working_dir}}/analyses
      
      # Check for precomputed values first (from parent recipe)
      precomputed_owner="{{_precomputed.repo_owner}}"
      precomputed_name="{{_precomputed.repo_name}}"
      repo_url="{{repo_url}}"
      
      if [ -n "$precomputed_owner" ] && [ -n "$precomputed_name" ]; then
        # Use precomputed values - skip URL parsing
        echo "{\"repo_url\": \"$repo_url\", \"owner\": \"$precomputed_owner\", \"repo_name\": \"$precomputed_name\", \"detected_from\": \"precomputed\", \"gh_available\": true, \"error\": null}"
        exit 0
      fi
      
      # Fall back to URL parsing for standalone execution
      if [ -z "$repo_url" ]; then
        repo_url=$(git remote get-url origin 2>/dev/null || echo "")
        detected_from="git_remote"
      else
        detected_from="provided"
      fi
      
      # Normalize URL and extract owner/repo
      normalized=$(echo "$repo_url" | sed 's/\.git$//' | sed 's|git@github.com:|https://github.com/|')
      owner=$(echo "$normalized" | sed 's|https://github.com/||' | cut -d'/' -f1)
      repo_name=$(echo "$normalized" | sed 's|https://github.com/||' | cut -d'/' -f2)
      
      # Check gh CLI
      if gh auth status >/dev/null 2>&1; then
        gh_available=true
      else
        gh_available=false
      fi
      
      if [ -z "$owner" ] || [ -z "$repo_name" ]; then
        echo "{\"error\": \"Could not parse repo URL: $repo_url\", \"gh_available\": $gh_available}"
      else
        echo "{\"repo_url\": \"$normalized\", \"owner\": \"$owner\", \"repo_name\": \"$repo_name\", \"detected_from\": \"$detected_from\", \"gh_available\": $gh_available, \"error\": null}"
      fi
    output: "repo_info"
    timeout: 120
    on_error: "fail"

  # ==========================================================================
  # Step 2: Parse date range (uses precomputed or LLM)
  # ==========================================================================
  # Optimization: Skip LLM call if parent provides _precomputed.date_since_iso
  - id: "parse-date-range"
    type: "bash"
    command: |
      # Check for precomputed date values from parent recipe
      precomputed_iso="{{_precomputed.date_since_iso}}"
      precomputed_date="{{_precomputed.date_since_date}}"
      precomputed_desc="{{_precomputed.date_description}}"
      
      if [ -n "$precomputed_iso" ]; then
        # Use precomputed values - NO LLM CALL NEEDED
        echo "{\"needs_llm\": \"false\", \"original\": \"{{date_range}}\", \"since_iso\": \"$precomputed_iso\", \"since_date\": \"$precomputed_date\", \"description\": \"$precomputed_desc\", \"source\": \"precomputed\"}"
      else
        # Signal that we need LLM parsing (standalone mode)
        echo "{\"needs_llm\": \"true\", \"date_range\": \"{{date_range}}\"}"
      fi
    output: "date_check"
    parse_json: true
    timeout: 10

  # Step 2b: LLM date parsing (only if precomputed not available)
  - id: "parse-date-range-llm"
    condition: "{{date_check.needs_llm}} == 'true'"
    agent: "foundation:zen-architect"
    mode: "ANALYZE"
    prompt: |
      Parse the natural language date range into ISO 8601 format for GitHub API.
      
      Input: "{{date_range}}"
      
      ## CRITICAL: Use LOCAL Timezone
      
      Run this to get local time AND timezone:
      ```bash
      date '+%Y-%m-%d %H:%M:%S %Z (UTC%:z)'
      ```
      
      The user's request is in THEIR local time. "Tuesday 3pm" means 3pm in their
      timezone, not UTC. Convert to UTC (Z suffix) for the API.
      
      ## CRITICAL: No End Date for Open-Ended Queries
      
      For "since X" queries, we only need a start time. Do NOT calculate an end time.
      This prevents timezone edge cases from missing recent commits.
      
      Interpretation (relative to user's local time):
      - "since yesterday" -> yesterday at 00:00:00 LOCAL, converted to UTC
      - "since Tuesday 3pm" -> Tuesday 15:00 LOCAL, converted to UTC
      - "last 7 days" -> 7 days ago at 00:00:00 LOCAL
      - "since 2024-12-01" -> 2024-12-01T00:00:00 in user's timezone
      
      Return:
      {
        "original": "{{date_range}}",
        "since_iso": "YYYY-MM-DDTHH:MM:SSZ",  // UTC time after timezone conversion
        "since_date": "YYYY-MM-DD",            // Local date
        "description": "human readable (e.g., 'since Tuesday 3pm PST (2024-12-31T23:00:00Z)')",
        "source": "llm_parsed"
      }
    output: "parsed_date_llm"
    parse_json: true
    timeout: 60

  # Step 2c: Use precomputed date (skips if LLM was needed)
  - id: "use-precomputed-date"
    condition: "{{date_check.needs_llm}} == 'false'"
    type: "bash"
    command: |
      echo '{"original": "{{date_check.original}}", "since_iso": "{{date_check.since_iso}}", "since_date": "{{date_check.since_date}}", "description": "{{date_check.description}}", "source": "precomputed"}'
    output: "parsed_date"
    parse_json: true
    timeout: 10

  # Step 2d: Use LLM-parsed date (only runs if LLM step ran)
  - id: "use-llm-date"
    condition: "{{date_check.needs_llm}} == 'true'"
    type: "bash"
    command: |
      echo '{"original": "{{parsed_date_llm.original}}", "since_iso": "{{parsed_date_llm.since_iso}}", "since_date": "{{parsed_date_llm.since_date}}", "description": "{{parsed_date_llm.description}}", "source": "llm_parsed"}'
    output: "parsed_date"
    parse_json: true
    timeout: 10

  # ==========================================================================
  # Step 3: Fetch commits (Bash - direct API calls)
  # ==========================================================================
  # BASH STEP - Fetch commits via GitHub API
  # Why bash: gh api + jq transforms are deterministic, no reasoning needed
  - id: "fetch-commits"
    type: "bash"
    command: |
      cd {{working_dir}}/analyses
      
      # Fetch commits from GitHub API
      gh api "repos/{{repo_info.owner}}/{{repo_info.repo_name}}/commits?since={{parsed_date.since_iso}}&per_page=100" \
        --jq '[.[] | {sha: .sha, message: .commit.message, author: .commit.author.name, date: .commit.author.date, url: .html_url}]' \
        2>/dev/null > {{repo_info.repo_name}}-commits.json || echo "[]" > {{repo_info.repo_name}}-commits.json
      
      # Use jq to safely construct output JSON (handles escaping of quotes/newlines in commit messages)
      jq -c '{count: length, commits: ., error: null}' {{repo_info.repo_name}}-commits.json 2>/dev/null || \
        echo '{"count": 0, "commits": [], "error": "Failed to parse commits"}'
    output: "commits_data"
    parse_json: true
    timeout: 300

  # ==========================================================================
  # Step 4: Fetch PRs (Bash - direct API calls)
  # ==========================================================================
  # BASH STEP - Fetch PRs via GitHub CLI
  # Why bash: gh pr list + jq filtering are deterministic, no reasoning needed
  - id: "fetch-prs"
    type: "bash"
    command: |
      cd {{working_dir}}/analyses
      
      # Fetch PRs from GitHub
      gh pr list --repo {{repo_info.owner}}/{{repo_info.repo_name}} --state all --limit 100 \
        --json number,title,state,author,createdAt,mergedAt,closedAt,additions,deletions,changedFiles,url \
        2>/dev/null > {{repo_info.repo_name}}-prs-raw.json || echo "[]" > {{repo_info.repo_name}}-prs-raw.json
      
      # Filter to date range and count states
      since_iso="{{parsed_date.since_iso}}"
      
      jq --arg since "$since_iso" '[.[] | select(.createdAt >= $since or .mergedAt >= $since or .closedAt >= $since)]' \
        {{repo_info.repo_name}}-prs-raw.json > {{repo_info.repo_name}}-prs.json 2>/dev/null || echo "[]" > {{repo_info.repo_name}}-prs.json
      
      # Use jq to safely construct output JSON with counts (handles escaping of quotes in PR titles)
      jq -c '{
        count: length,
        prs: .,
        merged: [.[] | select(.state == "MERGED")] | length,
        open: [.[] | select(.state == "OPEN")] | length,
        closed: [.[] | select(.state == "CLOSED")] | length,
        error: null
      }' {{repo_info.repo_name}}-prs.json 2>/dev/null || \
        echo '{"count": 0, "prs": [], "merged": 0, "open": 0, "closed": 0, "error": "Failed to parse PRs"}'
    output: "prs_data"
    parse_json: true
    timeout: 300

  # ==========================================================================
  # Step 5: Analyze commits (LLM for semantic analysis)
  # ==========================================================================
  # AGENT STEP - Understand commit impact and categorize changes
  # Why agent: Requires reading commit messages, inferring intent, categorizing impact
  - id: "analyze-commits"
    agent: "foundation:zen-architect"
    mode: "ANALYZE"
    prompt: |
      Analyze the commits for {{repo_info.repo_name}}.
      
      Commits count: {{commits_data.count}}
      Commits: {{commits_data.commits}}
      
      **If commits count is 0 or commits list is empty, return immediately:**
      ```json
      {
        "total_analyzed": 0,
        "by_impact": {"trivial": 0, "minor": 0, "moderate": 0, "significant": 0, "breaking": 0},
        "summaries": [],
        "needs_deep_dive": [],
        "themes": [],
        "skipped": true,
        "reason": "No commits in date range"
      }
      ```
      
      **Otherwise, for each commit:**
      
      For each commit (or batch if many):
      
      1. Fetch the diff using gh CLI:
      ```bash
      gh api "repos/{{repo_info.owner}}/{{repo_info.repo_name}}/commits/<SHA>" \
        --jq '{files: [.files[] | {filename, status, additions, deletions}], stats: .stats}'
      ```
      
      2. Analyze:
         - What changed (files, patterns)
         - Why (infer from commit message)
         - Impact level: trivial | minor | moderate | significant | breaking
         - Flag if needs_deep_dive (impact unclear from message + diff alone)
      
      For large numbers of commits (>20):
      - Group by author or by area (docs, src, tests)
      - Summarize patterns rather than individual commits
      - Still flag any that seem significant
      
      Return:
      {
        "total_analyzed": N,
        "by_impact": {
          "trivial": N,
          "minor": N,
          "moderate": N,
          "significant": N,
          "breaking": N
        },
        "summaries": [
          {
            "sha": "...",
            "summary": "...",
            "impact": "...",
            "needs_deep_dive": true|false,
            "files_changed": [...]
          }
        ],
        "needs_deep_dive": [list of SHAs needing deeper analysis],
        "themes": ["list of patterns/themes observed"]
      }
    output: "commit_analysis"
    parse_json: true
    timeout: 600

  # ==========================================================================
  # Step 6: Deep-dive unclear commits (optional LLM analysis)
  # ==========================================================================
  # AGENT STEP - Explore code context to understand unclear changes
  # Why agent: Requires git exploration, reading code, inferring impact
  - id: "deep-dive-unclear-commits"
    agent: "foundation:explorer"
    prompt: |
      Deep-dive analysis for commits with unclear impact in {{repo_info.repo_name}}.
      
      ## Inputs
      
      Include deep dive: {{include_deep_dive}}
      Commits needing analysis: {{commit_analysis.needs_deep_dive}}
      
      ## Logic
      
      **If include_deep_dive is false OR the needs_deep_dive list is empty:**
      Return immediately with:
      ```json
      {"deep_dives": [], "skipped": true, "reason": "No deep dive needed or disabled"}
      ```
      
      **Otherwise, for each commit SHA in needs_deep_dive:**
      
      1. If repo not cloned locally, clone it:
      ```bash
      gh repo clone {{repo_info.owner}}/{{repo_info.repo_name}} {{working_dir}}/repos/{{repo_info.repo_name}} -- --depth=50
      ```
      
      2. Checkout the commit and explore:
      ```bash
      cd {{working_dir}}/repos/{{repo_info.repo_name}}
      git show <SHA> --stat
      git show <SHA> -- <key files>
      ```
      
      3. For changed functions/classes, find:
         - What depends on them (grep for imports/usages)
         - Related tests (what behavior is expected)
         - Documentation (what's the intended purpose)
      
      4. Assess actual impact:
         - Is this a breaking change?
         - What's the blast radius?
         - Are there downstream consumers affected?
      
      Return enhanced analysis:
      {
        "deep_dives": [
          {
            "sha": "...",
            "original_assessment": "...",
            "revised_impact": "...",
            "dependencies_found": [...],
            "risk_assessment": "...",
            "notes": "..."
          }
        ],
        "skipped": false
      }
    output: "deep_dive_results"
    parse_json: true
    timeout: 600
    on_error: "continue"

  # ==========================================================================
  # Step 7: Analyze PRs (LLM for semantic analysis)
  # ==========================================================================
  # AGENT STEP - Categorize PRs by type and assess significance
  # Why agent: Requires reading PR titles/descriptions, inferring categories
  - id: "analyze-prs"
    agent: "foundation:zen-architect"
    mode: "ANALYZE"
    prompt: |
      Analyze the PRs for {{repo_info.repo_name}}.
      
      PRs count: {{prs_data.count}}
      PRs: {{prs_data.prs}}
      
      **If PRs count is 0 or PRs list is empty, return immediately:**
      ```json
      {
        "total_prs": 0,
        "merged": 0,
        "open": 0,
        "by_category": {},
        "notable_prs": [],
        "themes": [],
        "skipped": true,
        "reason": "No PRs in date range"
      }
      ```
      
      **Otherwise, for each PR:**
      
      For each PR:
      1. Categorize: feature | bugfix | docs | refactor | dependencies | other
      2. Assess scope: small (<50 lines) | medium (50-200) | large (>200)
      3. Note if merged, still open, or closed without merge
      
      Identify:
      - Major features added
      - Significant bug fixes
      - Breaking changes
      - PRs that might need attention (stale, contentious, large)
      
      Return:
      {
        "total_prs": N,
        "merged": N,
        "open": N,
        "by_category": { "feature": N, "bugfix": N, ... },
        "notable_prs": [
          {
            "number": N,
            "title": "...",
            "category": "...",
            "scope": "...",
            "summary": "...",
            "notable_because": "..."
          }
        ],
        "themes": ["patterns observed in PRs"]
      }
    output: "pr_analysis"
    parse_json: true
    timeout: 300

  # ==========================================================================
  # Step 8: Synthesize repo findings (LLM for comprehensive summary)
  # ==========================================================================
  # AGENT STEP - Combine all findings into cohesive narrative
  # Why agent: Requires weighing importance, prioritizing, creating narrative
  # v1.5.0: Added compact synthesis_input for aggregation (prevents context overflow)
  - id: "synthesize-repo-findings"
    agent: "foundation:zen-architect"
    mode: "ARCHITECT"
    prompt: |
      Synthesize all findings for {{repo_info.repo_name}}.
      
      Inputs:
      - Repo info: {{repo_info}}
      - Commits: {{commit_analysis}}
      - Deep dives: {{deep_dive_results}}
      - PRs: {{pr_analysis}}
      - Date range: {{date_range}} ({{parsed_date.description}})
      
      Create TWO outputs:
      
      ## 1. Full Analysis (repo_summary - for file storage)
      
      {
        "repo": "{{repo_info.repo_name}}",
        "owner": "{{repo_info.owner}}",
        "url": "{{repo_info.repo_url}}",
        "analysis_date": "<current timestamp>",
        "date_range": "{{parsed_date.description}}",
        
        "activity_summary": {
          "has_activity": true|false,
          "commits": N,
          "prs_total": N,
          "prs_merged": N,
          "prs_open": N
        },
        
        "impact_assessment": {
          "overall": "none|trivial|minor|moderate|significant|breaking",
          "by_level": { "trivial": N, ... }
        },
        
        "key_changes": [
          {
            "description": "...",
            "impact": "...",
            "type": "commit|pr",
            "references": ["sha or PR#"]
          }
        ],
        
        "themes": ["patterns and themes observed"],
        
        "risks": ["any risks or concerns identified"],
        
        "notable_items": [
          "breaking changes, security items, major features"
        ]
      }
      
      ## 2. Compact Summary (synthesis_input - for ecosystem aggregation, MAX 300 tokens)
      
      {
        "repo": "{{repo_info.repo_name}}",
        "commits": N,
        "prs": N,
        "impact": "none|trivial|minor|moderate|significant|breaking",
        "summary": "1-2 sentence summary of what changed",
        "highlights": ["max 3 bullet points of most important changes"],
        "risks": ["max 2 critical risks only, or empty"]
      }
      
      IMPORTANT: synthesis_input is for ecosystem reports with 30+ repos. Keep it COMPACT.
      
      Return JSON with BOTH fields:
      {
        "repo_summary": { ... full analysis ... },
        "synthesis_input": { ... compact summary ... }
      }
    output: "repo_findings"
    parse_json: true
    timeout: 300

  # ==========================================================================
  # Step 9: Write analysis files (GUARANTEED file write)
  # ==========================================================================
  # BASH STEP - Explicit file write since LLM file writes are non-deterministic
  # Why bash: Guarantees files are written, verifies success
  # v1.5.0: Returns compact synthesis_input as final_output for parent aggregation
  #         Handles both old format (direct object) and new format (wrapped)
  - id: "write-analysis-files"
    type: "bash"
    command: |
      set -euo pipefail
      
      mkdir -p {{working_dir}}/analyses
      
      # Check if input has new format (repo_summary field) or old format (direct object)
      # If .repo_summary exists, use it; otherwise treat whole object as the summary
      if printf '%s\n' '{{repo_findings}}' | jq -e '.repo_summary' >/dev/null 2>&1; then
        printf '%s\n' '{{repo_findings}}' | jq '.repo_summary' > {{working_dir}}/analyses/{{repo_info.repo_name}}-analysis.json.tmp
      else
        printf '%s\n' '{{repo_findings}}' > {{working_dir}}/analyses/{{repo_info.repo_name}}-analysis.json.tmp
      fi
      mv {{working_dir}}/analyses/{{repo_info.repo_name}}-analysis.json.tmp {{working_dir}}/analyses/{{repo_info.repo_name}}-analysis.json
      
      # Generate markdown summary using jq (avoids YAML heredoc issues)
      jq -r '
        "# " + .repo + " Repository Analysis\n\n" +
        "**Repository**: [" + .owner + "/" + .repo + "](" + .url + ")  \n" +
        "**Analysis Date**: " + .analysis_date + "  \n" +
        "**Period**: " + .date_range + "\n\n" +
        "## Activity Summary\n\n" +
        "| Metric | Count |\n|--------|-------|\n" +
        "| Commits | " + (.activity_summary.commits | tostring) + " |\n" +
        "| PRs Total | " + (.activity_summary.prs_total | tostring) + " |\n" +
        "| PRs Merged | " + (.activity_summary.prs_merged | tostring) + " |\n\n" +
        "## Impact Assessment: " + (.impact_assessment.overall | ascii_upcase) + "\n\n" +
        (if .impact_assessment.by_level then
          "| Level | Count |\n|-------|-------|\n" +
          (.impact_assessment.by_level | to_entries | map("| " + .key + " | " + (.value|tostring) + " |") | join("\n")) + "\n\n"
        else "" end) +
        "## Key Changes\n\n" +
        (if .key_changes and (.key_changes | length) > 0 then
          (.key_changes | to_entries | map(((.key + 1) | tostring) + ". " + .value.description + " (" + .value.impact + ")") | join("\n\n")) + "\n\n"
        else "No significant changes in this period.\n\n" end) +
        "## Themes\n\n" +
        (if .themes and (.themes | length) > 0 then
          (.themes | map("- " + .) | join("\n")) + "\n\n"
        else "- No major themes identified\n\n" end) +
        "## Risks\n\n" +
        (if .risks and (.risks | length) > 0 then
          (.risks | map("- " + .) | join("\n")) + "\n\n"
        else "- No significant risks identified\n\n" end) +
        "## Notable Items\n\n" +
        (if .notable_items and (.notable_items | length) > 0 then
          (.notable_items | map("- " + .) | join("\n"))
        else "- No notable items" end)
      ' {{working_dir}}/analyses/{{repo_info.repo_name}}-analysis.json > {{working_dir}}/analyses/{{repo_info.repo_name}}-summary.md
      
      # Verify files were written
      json_size=$(wc -c < "{{working_dir}}/analyses/{{repo_info.repo_name}}-analysis.json" | tr -d ' ')
      md_size=$(wc -c < "{{working_dir}}/analyses/{{repo_info.repo_name}}-summary.md" | tr -d ' ')
      
      echo "Files written:" >&2
      echo "  - {{repo_info.repo_name}}-analysis.json ($json_size bytes)" >&2
      echo "  - {{repo_info.repo_name}}-summary.md ($md_size bytes)" >&2
      
      # Return COMPACT synthesis_input as final output (for parent recipe aggregation)
      # Construct from analysis file to ensure consistent format regardless of LLM output
      jq -c '{
        repo: .repo,
        commits: .activity_summary.commits,
        prs: .activity_summary.prs_total,
        impact: .impact_assessment.overall,
        summary: (
          if .themes and (.themes | length) > 0 then
            (.themes[:2] | join("; "))
          else
            "Activity in " + .repo
          end
        ),
        highlights: (
          if .key_changes and (.key_changes | length) > 0 then
            [.key_changes[:3][] | .description[:80]]
          else
            []
          end
        ),
        risks: (
          if .risks and (.risks | length) > 0 then
            [.risks[:2][] | .[:60]]
          else
            []
          end
        )
      }' {{working_dir}}/analyses/{{repo_info.repo_name}}-analysis.json
    output: "final_output"
    parse_json: true
    timeout: 60
    on_error: "continue"  # Allow parent to handle failures via validation/retry
