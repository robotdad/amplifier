name: "ecosystem-activity-report"
description: "Analyze activity across the Amplifier ecosystem by discovering repos from MODULES.md"
version: "1.0.0"
author: "Amplifier"
tags: ["amplifier", "ecosystem", "github", "activity", "modules", "discovery"]

# Amplifier Ecosystem Activity Report
#
# This recipe is the top-level entry point for analyzing activity across
# the Amplifier ecosystem. It:
#
# 1. Discovers the current GitHub user (for filtering to your activity)
# 2. Parses date range from natural language
# 3. Reads MODULES.md to discover all ecosystem repos
# 4. Filters repos based on criteria (org, name pattern)
# 5. Calls the generic multi-repo recipe from @recipes bundle
#
# Usage (all your activity today):
#   amplifier run "execute recipe @amplifier:recipes/ecosystem-activity-report.yaml"
#
# Usage (all ecosystem activity since yesterday):
#   amplifier run 'execute recipe @amplifier:recipes/ecosystem-activity-report.yaml with context {
#     "author_filter": "",
#     "date_range": "since yesterday"
#   }'
#
# Usage (specific repos only):
#   amplifier run 'execute recipe @amplifier:recipes/ecosystem-activity-report.yaml with context {
#     "repo_filter": "amplifier-core|amplifier-foundation"
#   }'
#
# Requirements:
#   - gh CLI installed and authenticated
#   - recipes bundle loaded (provides generic repo-activity-analysis recipe)

context:
  # Date range (natural language) - defaults to today
  date_range: "today"
  
  # Filter to specific GitHub username (empty = discover current user, "all" = no filter)
  author_filter: ""
  
  # Filter repos by regex pattern (applied to repo name)
  # Examples: "amplifier-core", "amplifier-core|amplifier-foundation", "" (all)
  repo_filter: ""
  
  # Filter repos by GitHub org (default: microsoft)
  org_filter: "microsoft"
  
  # Working directory for intermediate files
  working_dir: "./ai_working"
  
  # Output report filename
  report_filename: "ecosystem-activity-report.md"
  
  # Default values for step outputs (prevents undefined variable errors)
  user_info: {"username": "", "discovered": false}
  parsed_date: {"original": "", "iso_since": "", "iso_until": "", "gh_format": ""}
  modules_content: {"path": "", "content": "", "error": null}
  discovered_repos: {"total": 0, "repos": []}
  filtered_repos: {"count": 0, "repos": [], "dropped": []}
  activity_checks: []
  active_repos: {"count": 0, "repos": [], "skipped": []}

steps:
  # ==========================================================================
  # Step 1: Setup working directory and discover GitHub user
  # ==========================================================================
  # Direct bash approach - no LLM reasoning needed for simple commands
  - id: "setup-and-discover-user"
    agent: "foundation:explorer"
    parse_json: true
    prompt: |
      Setup working directory and discover GitHub user. Execute these commands directly.
      
      ## Setup
      
      ```bash
      # Create all working directories upfront
      mkdir -p {{working_dir}}/{discovery,repos,reports}
      ```
      
      ## User Discovery
      
      Author filter setting: "{{author_filter}}"
      
      Execute this logic using bash:
      
      ```bash
      author_filter="{{author_filter}}"
      
      if [ "$author_filter" = "all" ] || [ "$author_filter" = "ALL" ]; then
        echo '{"username": "", "discovered": false, "filter_mode": "all"}'
      elif [ -z "$author_filter" ]; then
        username=$(gh api user --jq '.login' 2>/dev/null)
        if [ -n "$username" ]; then
          echo "{\"username\": \"$username\", \"discovered\": true, \"filter_mode\": \"current_user\"}"
        else
          echo '{"username": "", "discovered": false, "filter_mode": "all", "error": "Could not discover user"}'
        fi
      else
        echo "{\"username\": \"$author_filter\", \"discovered\": false, \"filter_mode\": \"specified\"}"
      fi
      ```
      
      Run the bash script and return its JSON output directly.
    output: "user_info"
    timeout: 60

  # ==========================================================================
  # Step 2: Parse date range (LLM for natural language flexibility)
  # ==========================================================================
  # LLM handles natural language interpretation - supports flexible input
  - id: "parse-date-range"
    agent: "foundation:explorer"
    parse_json: true
    prompt: |
      Parse the natural language date range into ISO 8601 format for GitHub API.
      
      ## Input
      
      Date range: "{{date_range}}"
      
      First, get the current date/time:
      ```bash
      date -u +%Y-%m-%dT%H:%M:%SZ
      ```
      
      ## Interpretation Guidelines
      
      | Input | Interpretation |
      |-------|---------------|
      | "today" | From midnight today (00:00:00Z) to now |
      | "yesterday" | From midnight yesterday to midnight today |
      | "since yesterday" | From midnight yesterday to now |
      | "past day", "last day", "last 24 hours" | Same as "since yesterday" |
      | "last 7 days", "past week", "this week" | From 7 days ago to now |
      | "last 2 weeks" | From 14 days ago to now |
      | "last month", "past 30 days" | From 30 days ago to now |
      | "this month" | From 1st of current month to now |
      | "since Monday" | From most recent Monday to now |
      | "since Dec 15" | From Dec 15 00:00:00Z to now |
      | "Dec 15-20" | From Dec 15 to Dec 20 23:59:59Z |
      | "since the 1st" | From 1st of current month to now |
      
      Handle variations gracefully:
      - "the past few days" → interpret as ~3 days
      - "recently" → interpret as last 7 days
      - Typos like "yeterday" → interpret as "yesterday"
      
      ## Output
      
      Return JSON:
      {
        "original": "{{date_range}}",
        "iso_since": "YYYY-MM-DDTHH:MM:SSZ",
        "iso_until": "YYYY-MM-DDTHH:MM:SSZ",
        "gh_format": "YYYY-MM-DD",
        "description": "human readable description (e.g., 'since yesterday (2024-12-28 to now)')"
      }
    output: "parsed_date"
    timeout: 60

  # ==========================================================================
  # Step 3: Fetch MODULES.md
  # ==========================================================================
  # Direct bash - fetch from GitHub API (most reliable source of truth)
  - id: "fetch-modules-md"
    agent: "foundation:explorer"
    parse_json: true
    prompt: |
      Fetch MODULES.md from GitHub and save to working directory.
      
      ## Execute
      
      ```bash
      # Fetch from GitHub API (authoritative source)
      gh api repos/microsoft/amplifier/contents/docs/MODULES.md \
        --jq '.content' | base64 -d > {{working_dir}}/discovery/MODULES.md
      
      # Get file info
      if [ -f "{{working_dir}}/discovery/MODULES.md" ]; then
        len=$(wc -c < "{{working_dir}}/discovery/MODULES.md" | tr -d ' ')
        echo "{\"path\": \"github-api\", \"saved_to\": \"{{working_dir}}/discovery/MODULES.md\", \"content_length\": $len, \"error\": null}"
      else
        echo '{"path": null, "saved_to": null, "content_length": 0, "error": "Failed to fetch MODULES.md"}'
      fi
      ```
      
      Run the commands and return the JSON output.
    output: "modules_content"
    timeout: 120

  # ==========================================================================
  # Step 4: Extract repos using grep (reliable extraction)
  # ==========================================================================
  # Uses grep/sed to extract ALL GitHub URLs - no LLM parsing
  - id: "extract-repos-grep"
    agent: "foundation:explorer"
    parse_json: true
    prompt: |
      Extract all GitHub repository URLs from MODULES.md using grep.
      
      ## Execute
      
      Run this bash script to extract and format repos:
      
      ```bash
      cd {{working_dir}}/discovery
      
      # Extract all GitHub URLs, normalize, deduplicate, and format as JSON
      grep -oE 'https://github\.com/[^/]+/[^/)\"'\''#@\` ]+' MODULES.md \
        | sed 's/\.git$//' \
        | sed 's/[[:space:]]*$//' \
        | sort -u \
        | while read url; do
            owner=$(echo "$url" | cut -d'/' -f4)
            repo=$(echo "$url" | cut -d'/' -f5)
            echo "{\"owner\": \"$owner\", \"name\": \"$repo\", \"url\": \"$url\"}"
          done > repos-extracted.jsonl
      
      # Count and format as JSON array
      count=$(wc -l < repos-extracted.jsonl | tr -d ' ')
      
      # Build final JSON
      echo "{" > all-repos.json
      echo "  \"total\": $count," >> all-repos.json
      echo "  \"repos\": [" >> all-repos.json
      
      # Add each repo with proper comma handling
      i=0
      while read line; do
        i=$((i + 1))
        if [ $i -eq $count ]; then
          echo "    $line" >> all-repos.json
        else
          echo "    $line," >> all-repos.json
        fi
      done < repos-extracted.jsonl
      
      echo "  ]" >> all-repos.json
      echo "}" >> all-repos.json
      
      cat all-repos.json
      ```
      
      Run the script and return the JSON output.
    output: "discovered_repos"
    timeout: 120

  # ==========================================================================
  # Step 4b: Verify extraction (optional LLM check)
  # ==========================================================================
  # LLM reviews grep output vs source to catch any issues
  - id: "verify-extraction"
    agent: "foundation:explorer"
    parse_json: true
    prompt: |
      Verify the grep extraction caught all repos correctly.
      
      ## Extracted Repos
      
      Total found: {{discovered_repos.total}}
      Repos: {{discovered_repos.repos}}
      
      ## Quick Sanity Check
      
      Run a quick verification:
      
      ```bash
      cd {{working_dir}}/discovery
      
      # Count GitHub URLs in source (rough check)
      source_count=$(grep -c 'github.com/' MODULES.md || echo "0")
      extracted_count={{discovered_repos.total}}
      
      echo "Source mentions: $source_count"
      echo "Extracted unique: $extracted_count"
      
      # Check for key repos we expect to find
      expected="amplifier amplifier-core amplifier-foundation amplifier-app-cli"
      missing=""
      for repo in $expected; do
        if ! grep -q "\"name\": \"$repo\"" all-repos.json; then
          missing="$missing $repo"
        fi
      done
      
      if [ -n "$missing" ]; then
        echo "WARNING: Missing expected repos:$missing"
        echo '{"verified": false, "issue": "missing_expected", "missing": "'$missing'"}'
      else
        echo '{"verified": true, "source_mentions": '$source_count', "extracted_unique": '$extracted_count'}'
      fi
      ```
      
      Run the verification and return the result.
    output: "extraction_verified"
    timeout: 60

  # ==========================================================================
  # Step 5: Filter repos using jq
  # ==========================================================================
  # Direct bash/jq filtering - no LLM needed
  - id: "filter-repos"
    agent: "foundation:explorer"
    parse_json: true
    prompt: |
      Filter discovered repos using jq.
      
      ## Execute
      
      ```bash
      cd {{working_dir}}/discovery
      
      org_filter="{{org_filter}}"
      repo_filter="{{repo_filter}}"
      
      # Start with all repos
      cp all-repos.json filtered-repos.json
      
      # Apply org filter if set
      if [ -n "$org_filter" ]; then
        jq --arg org "$org_filter" '{
          total: ([.repos[] | select(.owner == $org)] | length),
          repos: [.repos[] | select(.owner == $org)]
        }' all-repos.json > filtered-repos.json
      fi
      
      # Apply repo name filter if set (grep-style regex)
      if [ -n "$repo_filter" ]; then
        jq --arg pattern "$repo_filter" '{
          total: ([.repos[] | select(.name | test($pattern; "i"))] | length),
          repos: [.repos[] | select(.name | test($pattern; "i"))]
        }' filtered-repos.json > filtered-repos-tmp.json
        mv filtered-repos-tmp.json filtered-repos.json
      fi
      
      # Build output with dropped repos
      original=$(jq '.total' all-repos.json)
      filtered=$(jq '.total' filtered-repos.json)
      
      # Get dropped repo names
      dropped=$(jq -r '.repos[].name' all-repos.json | while read name; do
        if ! jq -e --arg n "$name" '.repos[] | select(.name == $n)' filtered-repos.json >/dev/null 2>&1; then
          echo "$name"
        fi
      done | jq -R -s 'split("\n") | map(select(length > 0))')
      
      # Final output
      jq --argjson orig "$original" --argjson dropped "$dropped" \
        --arg org "$org_filter" --arg pattern "$repo_filter" '{
        original_count: $orig,
        count: .total,
        filters: {org: $org, repo_pattern: $pattern},
        repos: .repos,
        dropped: $dropped
      }' filtered-repos.json
      ```
      
      Run the script and return the JSON output.
    output: "filtered_repos"
    timeout: 60

  # ==========================================================================
  # Step 6: Quick activity check (pre-filter)
  # ==========================================================================
  # Direct bash/gh API calls - no LLM reasoning needed
  - id: "quick-activity-check"
    foreach: "{{filtered_repos.repos}}"
    as: "repo"
    parallel: true
    collect: "activity_checks"
    agent: "foundation:explorer"
    parse_json: true
    prompt: |
      Quick activity check using direct API calls.
      
      ## Execute
      
      ```bash
      filter_mode="{{user_info.filter_mode}}"
      
      if [ "$filter_mode" = "all" ]; then
        # No user filter - mark all as active
        echo '{"repo": "{{repo.name}}", "owner": "{{repo.owner}}", "url": "{{repo.url}}", "commits": -1, "prs": -1, "has_activity": true, "skipped_check": true}'
      else
        # Check commits (direct API - always fresh)
        commits=$(gh api "repos/{{repo.owner}}/{{repo.name}}/commits?author={{user_info.username}}&since={{parsed_date.iso_since}}&per_page=100" --jq 'length' 2>/dev/null || echo "0")
        
        # Check PRs with any involvement
        prs=$(gh api "search/issues?q=involves:{{user_info.username}}+repo:{{repo.owner}}/{{repo.name}}+type:pr+updated:>{{parsed_date.gh_format}}" --jq '.total_count' 2>/dev/null || echo "0")
        
        # Determine activity
        if [ "$commits" -gt 0 ] || [ "$prs" -gt 0 ]; then
          has_activity=true
        else
          has_activity=false
        fi
        
        echo "{\"repo\": \"{{repo.name}}\", \"owner\": \"{{repo.owner}}\", \"url\": \"{{repo.url}}\", \"commits\": $commits, \"prs\": $prs, \"has_activity\": $has_activity}"
      fi
      ```
      
      Run the script and return the JSON output directly.
    output: "activity_result"
    timeout: 60
    on_error: "continue"

  # ==========================================================================
  # Step 7: Filter to repos with activity
  # ==========================================================================
  # Direct jq filtering of activity check results
  - id: "filter-to-active"
    agent: "foundation:explorer"
    parse_json: true
    prompt: |
      Filter activity check results using jq.
      
      ## Input
      
      Activity checks (JSON array): {{activity_checks}}
      
      ## Execute
      
      ```bash
      cd {{working_dir}}/discovery
      
      # Save activity checks to file for processing
      cat > activity-checks.json << 'ACTIVITY_EOF'
      {{activity_checks}}
      ACTIVITY_EOF
      
      # Filter to active repos and build output
      jq '{
        count: ([.[] | select(.has_activity == true)] | length),
        repos: [.[] | select(.has_activity == true) | {owner, name: .repo, url, commits, prs}],
        skipped: [.[] | select(.has_activity == false) | .repo],
        skipped_count: ([.[] | select(.has_activity == false)] | length)
      }' activity-checks.json > active-repos.json
      
      cat active-repos.json
      ```
      
      Run the script and return the JSON output.
    output: "active_repos"
    timeout: 60

  # ==========================================================================
  # Step 8: Show discovery summary (LLM for user-facing formatting)
  # ==========================================================================
  # LLM creates a well-formatted, contextual summary for the user
  - id: "discovery-summary"
    agent: "foundation:explorer"
    prompt: |
      Create a clear, well-formatted discovery summary for the user.
      
      ## Discovery Results
      
      **User**: {{user_info.username}} ({{user_info.filter_mode}})
      **Date Range**: {{parsed_date.description}}
      **Total repos discovered**: {{discovered_repos.total}}
      **After filtering**: {{filtered_repos.count}}
      **With activity**: {{active_repos.count}}
      **Skipped (no activity)**: {{active_repos.skipped_count}}
      
      Active repos to analyze:
      {{active_repos.repos}}
      
      Skipped repos (no activity in date range):
      {{active_repos.skipped}}
      
      ## Format Requirements
      
      Print a clear summary to the console:
      
      ```
      ============================================================
      AMPLIFIER ECOSYSTEM ACTIVITY REPORT
      ============================================================
      User:       [username] ([filter mode])
      Date Range: [human readable description]
      
      Discovery:  [total] repos found → [filtered] after org filter → [active] with activity
      
      Analyzing ([N] repos):
        • [repo1] - [X commits, Y PRs]
        • [repo2] - [X commits, Y PRs]
        ...
      
      Skipped ([N] repos - no activity):
        [repo1], [repo2], ...
      
      Estimated time: ~[N] minutes (1 min per repo)
      ============================================================
      ```
      
      Make the output scannable and informative. Group repos logically if many.
      
      Return: {"displayed": true, "repos_to_analyze": [list of repo names]}
    output: "summary"
    timeout: 60

  # ==========================================================================
  # Step 9: Analyze each repo using generic recipe
  # ==========================================================================
  # Only analyzes repos that passed the quick activity check
  - id: "analyze-repos"
    foreach: "{{active_repos.repos}}"
    as: "repo"
    parallel: false
    collect: "repo_analyses"
    type: "recipe"
    recipe: "@recipes:examples/repo-activity-analysis.yaml"
    context:
      repo_url: "{{repo.url}}"
      date_range: "{{parsed_date.description}}"
      working_dir: "{{working_dir}}"
      include_deep_dive: false
    output: "single_repo_result"
    timeout: 600
    on_error: "continue"

  # ==========================================================================
  # Step 10: Synthesize final report
  # ==========================================================================
  - id: "synthesize-report"
    agent: "foundation:zen-architect"
    mode: "ARCHITECT"
    prompt: |
      Create the final Amplifier Ecosystem Activity Report.
      
      ## Input Data
      
      User: {{user_info.username}} ({{user_info.filter_mode}})
      Date Range: {{parsed_date.description}}
      Repos analyzed: {{active_repos.count}}
      Repos skipped (no activity): {{active_repos.skipped}}
      
      Individual analyses: {{repo_analyses}}
      
      ## Report Structure
      
      Create comprehensive markdown report:
      
      ```markdown
      # Amplifier Ecosystem Activity Report
      
      **Generated**: [current timestamp]
      **User**: {{user_info.username}}
      **Date Range**: {{parsed_date.description}}
      **Repos Analyzed**: {{active_repos.count}}
      
      ---
      
      ## Executive Summary
      
      - **Total commits**: N
      - **Total PRs**: N  
      - **Repos with activity**: N of {{active_repos.count}}
      
      ### Key Highlights
      
      [2-3 bullet points of most significant changes]
      
      ---
      
      ## Activity by Repository
      
      [For each repo WITH activity, ordered by activity level:]
      
      ### {repo_name}
      
      **Commits**: N | **PRs**: N
      
      **Summary**: [1-2 sentence summary]
      
      **Key Changes**:
      - [Notable changes]
      
      ---
      
      ## Cross-Cutting Observations
      
      [Patterns observed across repos - coordinated changes, themes]
      
      ## Repositories with No Activity
      
      [List repos with no changes in date range]
      
      ---
      *Generated by @amplifier:recipes/ecosystem-activity-report.yaml*
      ```
      
      ## Output
      
      Write report to: {{working_dir}}/reports/{{report_filename}}
      
      Also print the full report to console.
      
      Return the report content.
    output: "final_report"
    timeout: 600

  # ==========================================================================
  # Step 11: Completion and cleanup
  # ==========================================================================
  # Print summary, cleanup temp files, keep only final report
  - id: "complete"
    agent: "foundation:explorer"
    prompt: |
      Finalize, show completion status, and cleanup temp files.
      
      ## Execute
      
      ```bash
      # Get report path
      report_path=$(realpath {{working_dir}}/reports/{{report_filename}} 2>/dev/null || echo "{{working_dir}}/reports/{{report_filename}}")
      
      # Print completion summary
      cat << 'COMPLETE_EOF'
      ============================================================
      REPORT COMPLETE
      ============================================================
      COMPLETE_EOF
      echo "User:   {{user_info.username}}"
      echo "Range:  {{parsed_date.description}}"
      echo "Repos:  {{active_repos.count}} analyzed, {{active_repos.skipped_count}} skipped"
      echo "Report: $report_path"
      echo "============================================================"
      
      # Cleanup temporary files (keep reports/)
      echo ""
      echo "Cleaning up temporary files..."
      
      # Remove discovery intermediate files
      rm -rf {{working_dir}}/discovery
      
      # Remove any cloned repos (if any were created)
      rm -rf {{working_dir}}/repos
      
      # Show what remains
      echo "Remaining in {{working_dir}}:"
      ls -la {{working_dir}}/
      
      echo ""
      echo '{"completed": true, "report_path": "'$report_path'", "cleanup": "done"}'
      ```
      
      Run the script and return the JSON output.
    output: "completion"
    timeout: 60
