name: "ecosystem-activity-report"
description: "Analyze activity across the Amplifier ecosystem by discovering repos from MODULES.md"
version: "1.2.0"
author: "Amplifier"
tags: ["amplifier", "ecosystem", "github", "activity", "modules", "discovery"]

# Amplifier Ecosystem Activity Report
#
# This recipe is the top-level entry point for analyzing activity across
# the Amplifier ecosystem. It:
#
# 1. Discovers the current GitHub user (for filtering to your activity)
# 2. Parses date range from natural language
# 3. Reads MODULES.md to discover all ecosystem repos
# 4. Filters repos based on criteria (org, name pattern)
# 5. Calls the generic multi-repo recipe from @recipes bundle
#
# v1.2.0: Performance optimizations
#         - parallel: true default for ~4x faster execution
#         - Pass precomputed values to sub-recipes (eliminates 12 redundant LLM calls)
#         - Convert discovery-summary to bash (faster, deterministic)
#         - Add parallel_analysis toggle for user control
#
# v1.1.0: Converted bash-heavy steps to type: "bash" for efficiency
#         (no LLM overhead for deterministic shell commands)

recursion:
  max_depth: 3
  max_total_steps: 200
#
# Usage (all your activity today):
#   amplifier run "execute recipe @amplifier:recipes/ecosystem-activity-report.yaml"
#
# Usage (all ecosystem activity since yesterday):
#   amplifier run 'execute recipe @amplifier:recipes/ecosystem-activity-report.yaml with context {
#     "author_filter": "",
#     "date_range": "since yesterday"
#   }'
#
# Usage (specific repos only):
#   amplifier run 'execute recipe @amplifier:recipes/ecosystem-activity-report.yaml with context {
#     "repo_filter": "amplifier-core|amplifier-foundation"
#   }'
#
# Requirements:
#   - gh CLI installed and authenticated
#   - recipes bundle loaded (provides generic repo-activity-analysis recipe)

context:
  # Date range (natural language) - defaults to today
  date_range: "today"
  
  # Filter to specific GitHub username (empty = discover current user, "all" = no filter)
  author_filter: ""
  
  # Filter repos by regex pattern (applied to repo name)
  # Examples: "amplifier-core", "amplifier-core|amplifier-foundation", "" (all)
  repo_filter: ""
  
  # Filter repos by GitHub org (default: microsoft)
  org_filter: "microsoft"
  
  # Working directory for intermediate files
  working_dir: "./ai_working"
  
  # Output report filename
  report_filename: "ecosystem-activity-report.md"
  
  # Parallel analysis mode - set false if hitting rate limits or debugging
  parallel_analysis: true
  
  # Default values for step outputs (prevents undefined variable errors)
  user_info: {"username": "", "discovered": false}
  parsed_date: {"original": "", "iso_since": "", "iso_until": "", "gh_format": ""}
  modules_content: {"path": "", "content": "", "error": null}
  discovered_repos: {"total": 0, "repos": []}
  filtered_repos: {"count": 0, "repos": [], "dropped": []}
  activity_checks: []
  active_repos: {"count": 0, "repos": [], "skipped": []}

steps:
  # ==========================================================================
  # Step 0: Validate prerequisites (fail fast if tools missing)
  # ==========================================================================
  # BASH STEP - Check required tools before starting expensive operations
  # Why bash: Deterministic checks, no LLM needed, fail fast on missing deps
  - id: "validate-prerequisites"
    type: "bash"
    command: |
      set -euo pipefail
      
      errors=""
      warnings=""
      
      # Check gh CLI
      if ! command -v gh &>/dev/null; then
        errors="${errors}ERROR: gh CLI not found. Install: https://cli.github.com/\n"
      elif ! gh auth status &>/dev/null; then
        errors="${errors}ERROR: gh CLI not authenticated. Run: gh auth login\n"
      fi
      
      # Check jq
      if ! command -v jq &>/dev/null; then
        errors="${errors}ERROR: jq not found. Install: brew install jq (macOS) or apt install jq (Linux)\n"
      fi
      
      # Check base64 (usually available, but verify)
      if ! command -v base64 &>/dev/null; then
        errors="${errors}ERROR: base64 not found (required for decoding API responses)\n"
      fi
      
      # Optional: Check for rate limiting issues
      remaining=$(gh api rate_limit --jq '.resources.core.remaining' 2>/dev/null || echo "unknown")
      if [ "$remaining" != "unknown" ] && [ "$remaining" -lt 100 ]; then
        warnings="${warnings}WARNING: GitHub API rate limit low ($remaining remaining)\n"
      fi
      
      # Output results
      if [ -n "$errors" ]; then
        printf "$errors" >&2
        printf "$warnings" >&2
        echo '{"valid": false, "errors": "Missing prerequisites - see stderr"}'
        exit 1
      else
        if [ -n "$warnings" ]; then
          printf "$warnings" >&2
        fi
        echo "{\"valid\": true, \"gh_version\": \"$(gh --version | head -1)\", \"jq_version\": \"$(jq --version)\", \"rate_limit_remaining\": \"$remaining\"}"
      fi
    output: "prerequisites"
    timeout: 30
    on_error: "fail"

  # ==========================================================================
  # Step 1: Setup working directory and discover GitHub user
  # ==========================================================================
  # BASH STEP - No LLM needed for shell commands and API calls
  # Why bash: mkdir, gh api user are deterministic operations
  - id: "setup-and-discover-user"
    type: "bash"
    command: |
      set -euo pipefail
      
      # Create all working directories upfront
      mkdir -p {{working_dir}}/{discovery,repos,reports}
      
      # User Discovery
      author_filter="{{author_filter}}"
      
      if [ "$author_filter" = "all" ] || [ "$author_filter" = "ALL" ]; then
        echo '{"username": "", "discovered": false, "filter_mode": "all"}'
      elif [ -z "$author_filter" ]; then
        username=$(gh api user --jq '.login' 2>/dev/null)
        if [ -n "$username" ]; then
          echo "{\"username\": \"$username\", \"discovered\": true, \"filter_mode\": \"current_user\"}"
        else
          echo '{"username": "", "discovered": false, "filter_mode": "all", "error": "Could not discover user"}'
        fi
      else
        echo "{\"username\": \"$author_filter\", \"discovered\": false, \"filter_mode\": \"specified\"}"
      fi
    output: "user_info"
    timeout: 60

  # ==========================================================================
  # Step 2: Parse date range (LLM for natural language flexibility)
  # ==========================================================================
  # AGENT STEP - LLM handles natural language interpretation
  - id: "parse-date-range"
    agent: "foundation:explorer"
    prompt: |
      Parse the natural language date range into ISO 8601 format for GitHub API.
      
      ## Input
      
      Date range: "{{date_range}}"
      
      First, get the current date/time:
      ```bash
      date -u +%Y-%m-%dT%H:%M:%SZ
      ```
      
      ## Interpretation Guidelines
      
      | Input | Interpretation |
      |-------|---------------|
      | "today" | From midnight today (00:00:00Z) to now |
      | "yesterday" | From midnight yesterday to midnight today |
      | "since yesterday" | From midnight yesterday to now |
      | "past day", "last day", "last 24 hours" | Same as "since yesterday" |
      | "last 7 days", "past week", "this week" | From 7 days ago to now |
      | "last 2 weeks" | From 14 days ago to now |
      | "last month", "past 30 days" | From 30 days ago to now |
      | "this month" | From 1st of current month to now |
      | "since Monday" | From most recent Monday to now |
      | "since Dec 15" | From Dec 15 00:00:00Z to now |
      | "Dec 15-20" | From Dec 15 to Dec 20 23:59:59Z |
      | "since the 1st" | From 1st of current month to now |
      
      Handle variations gracefully:
      - "the past few days" → interpret as ~3 days
      - "recently" → interpret as last 7 days
      - Typos like "yeterday" → interpret as "yesterday"
      
      ## Output
      
      Return JSON:
      {
        "original": "{{date_range}}",
        "iso_since": "YYYY-MM-DDTHH:MM:SSZ",
        "iso_until": "YYYY-MM-DDTHH:MM:SSZ",
        "gh_format": "YYYY-MM-DD",
        "description": "human readable description (e.g., 'since yesterday (2024-12-28 to now)')"
      }
    output: "parsed_date"
    parse_json: true
    timeout: 60

  # ==========================================================================
  # Step 3: Fetch MODULES.md
  # ==========================================================================
  # BASH STEP - Direct GitHub API fetch, no interpretation needed
  # Why bash: gh api + base64 decode is deterministic, no reasoning required
  - id: "fetch-modules-md"
    type: "bash"
    command: |
      set -euo pipefail
      
      # Fetch from GitHub API (authoritative source)
      gh api repos/microsoft/amplifier/contents/docs/MODULES.md \
        --jq '.content' | base64 -d > {{working_dir}}/discovery/MODULES.md
      
      # Get file info
      if [ -f "{{working_dir}}/discovery/MODULES.md" ]; then
        len=$(wc -c < "{{working_dir}}/discovery/MODULES.md" | tr -d ' ')
        echo "{\"path\": \"github-api\", \"saved_to\": \"{{working_dir}}/discovery/MODULES.md\", \"content_length\": $len, \"error\": null}"
      else
        echo '{"path": null, "saved_to": null, "content_length": 0, "error": "Failed to fetch MODULES.md"}'
      fi
    output: "modules_content"
    timeout: 120

  # ==========================================================================
  # Step 4: Extract repos using grep (reliable extraction)
  # ==========================================================================
  # BASH STEP - Regex extraction from structured markdown
  # Why bash: grep/sed/sort are faster and more reliable than LLM for URL extraction
  - id: "extract-repos-grep"
    type: "bash"
    command: |
      cd {{working_dir}}/discovery
      
      # Extract all GitHub URLs, normalize, deduplicate, and format as JSON
      grep -oE 'https://github\.com/[^/]+/[^/)"'"'"'#@` ]+' MODULES.md \
        | sed 's/\.git$//' \
        | sed 's/[[:space:]]*$//' \
        | sort -u \
        | while read url; do
            owner=$(echo "$url" | cut -d'/' -f4)
            repo=$(echo "$url" | cut -d'/' -f5)
            echo "{\"owner\": \"$owner\", \"name\": \"$repo\", \"url\": \"$url\"}"
          done > repos-extracted.jsonl
      
      # Count and format as JSON array
      count=$(wc -l < repos-extracted.jsonl | tr -d ' ')
      
      # Build final JSON
      echo "{" > all-repos.json
      echo "  \"total\": $count," >> all-repos.json
      echo "  \"repos\": [" >> all-repos.json
      
      # Add each repo with proper comma handling
      i=0
      while read line; do
        i=$((i + 1))
        if [ $i -eq $count ]; then
          echo "    $line" >> all-repos.json
        else
          echo "    $line," >> all-repos.json
        fi
      done < repos-extracted.jsonl
      
      echo "  ]" >> all-repos.json
      echo "}" >> all-repos.json
      
      cat all-repos.json
    output: "discovered_repos"
    timeout: 120

  # ==========================================================================
  # Step 4b: Verify extraction
  # ==========================================================================
  # BASH STEP - Sanity check that expected repos are present
  # Why bash: Simple string matching, no semantic understanding needed
  - id: "verify-extraction"
    type: "bash"
    command: |
      cd {{working_dir}}/discovery
      
      # Count GitHub URLs in source (rough check)
      source_count=$(grep -c 'github.com/' MODULES.md || echo "0")
      extracted_count={{discovered_repos.total}}
      
      echo "Source mentions: $source_count"
      echo "Extracted unique: $extracted_count"
      
      # Check for key repos we expect to find
      expected="amplifier amplifier-core amplifier-foundation amplifier-app-cli"
      missing=""
      for repo in $expected; do
        if ! grep -q "\"name\": \"$repo\"" all-repos.json; then
          missing="$missing $repo"
        fi
      done
      
      if [ -n "$missing" ]; then
        echo "WARNING: Missing expected repos:$missing"
        echo '{"verified": false, "issue": "missing_expected", "missing": "'$missing'"}'
      else
        echo '{"verified": true, "source_mentions": '$source_count', "extracted_unique": '$extracted_count'}'
      fi
    output: "extraction_verified"
    timeout: 60

  # ==========================================================================
  # Step 5: Filter repos using jq
  # ==========================================================================
  # BASH STEP - jq filtering, no LLM needed
  - id: "filter-repos"
    type: "bash"
    command: |
      cd {{working_dir}}/discovery
      
      org_filter="{{org_filter}}"
      repo_filter="{{repo_filter}}"
      
      # Start with all repos
      cp all-repos.json filtered-repos.json
      
      # Apply org filter if set
      if [ -n "$org_filter" ]; then
        jq --arg org "$org_filter" '{
          total: ([.repos[] | select(.owner == $org)] | length),
          repos: [.repos[] | select(.owner == $org)]
        }' all-repos.json > filtered-repos.json
      fi
      
      # Apply repo name filter if set (grep-style regex)
      if [ -n "$repo_filter" ]; then
        jq --arg pattern "$repo_filter" '{
          total: ([.repos[] | select(.name | test($pattern; "i"))] | length),
          repos: [.repos[] | select(.name | test($pattern; "i"))]
        }' filtered-repos.json > filtered-repos-tmp.json
        mv filtered-repos-tmp.json filtered-repos.json
      fi
      
      # Build output with dropped repos
      original=$(jq '.total' all-repos.json)
      filtered=$(jq '.total' filtered-repos.json)
      
      # Get dropped repo names
      dropped=$(jq -r '.repos[].name' all-repos.json | while read name; do
        if ! jq -e --arg n "$name" '.repos[] | select(.name == $n)' filtered-repos.json >/dev/null 2>&1; then
          echo "$name"
        fi
      done | jq -R -s 'split("\n") | map(select(length > 0))')
      
      # Final output
      jq --argjson orig "$original" --argjson dropped "$dropped" \
        --arg org "$org_filter" --arg pattern "$repo_filter" '{
        original_count: $orig,
        count: .total,
        filters: {org: $org, repo_pattern: $pattern},
        repos: .repos,
        dropped: $dropped
      }' filtered-repos.json
    output: "filtered_repos"
    timeout: 60

  # ==========================================================================
  # Step 6: Quick activity check (BATCH - bash loop with rate limiting)
  # ==========================================================================
  # BASH STEP - Batch API calls to check for activity
  # Why bash: gh api calls are deterministic, rate limiting is simple loop logic
  - id: "quick-activity-check"
    type: "bash"
    command: |
      cd {{working_dir}}/discovery
      
      # Save repos for processing (printf safer than heredoc in YAML)
      printf '%s\n' '{{filtered_repos.repos}}' > repos-to-check.json
      
      filter_mode="{{user_info.filter_mode}}"
      username="{{user_info.username}}"
      since_iso="{{parsed_date.iso_since}}"
      gh_date="{{parsed_date.gh_format}}"
      
      # Process all repos in a bash loop with rate limiting
      echo "["
      first=true
      
      jq -c '.[]' repos-to-check.json | while read repo_json; do
        owner=$(echo "$repo_json" | jq -r '.owner')
        name=$(echo "$repo_json" | jq -r '.name')
        url=$(echo "$repo_json" | jq -r '.url')
        
        # Comma handling for JSON array
        if [ "$first" = "true" ]; then
          first=false
        else
          echo ","
          # Rate limiting: small delay between repos to avoid API limits
          sleep 0.5
        fi
        
        if [ "$filter_mode" = "all" ]; then
          # No user filter - mark all as active (skip API calls)
          echo "{\"repo\": \"$name\", \"owner\": \"$owner\", \"url\": \"$url\", \"commits\": -1, \"prs\": -1, \"has_activity\": true, \"skipped_check\": true}"
        else
          # Check commits
          commits=$(gh api "repos/$owner/$name/commits?author=$username&since=$since_iso&per_page=100" --jq 'length' 2>/dev/null || echo "0")
          
          # Brief pause between API calls
          sleep 0.3
          
          # Check PRs - use simpler query to avoid search API rate limits
          prs=$(gh api "repos/$owner/$name/pulls?state=all&per_page=100" --jq "[.[] | select(.user.login == \"$username\" and .updated_at >= \"$since_iso\")] | length" 2>/dev/null || echo "0")
          
          # Determine activity
          if [ "$commits" -gt 0 ] 2>/dev/null || [ "$prs" -gt 0 ] 2>/dev/null; then
            has_activity=true
          else
            has_activity=false
          fi
          
          echo "{\"repo\": \"$name\", \"owner\": \"$owner\", \"url\": \"$url\", \"commits\": $commits, \"prs\": $prs, \"has_activity\": $has_activity}"
        fi
      done
      
      echo "]"
    output: "activity_checks"
    timeout: 900

  # ==========================================================================
  # Step 7: Filter to repos with activity
  # ==========================================================================
  # BASH STEP - Filter JSON array by boolean field
  # Why bash: jq filtering is deterministic, no reasoning needed
  - id: "filter-to-active"
    type: "bash"
    command: |
      cd {{working_dir}}/discovery
      
      # Save activity checks to file for processing (printf safer than heredoc in YAML)
      printf '%s\n' '{{activity_checks}}' > activity-checks.json
      
      # Filter to active repos and build output
      jq '{
        count: ([.[] | select(.has_activity == true)] | length),
        repos: [.[] | select(.has_activity == true) | {owner, name: .repo, url, commits, prs}],
        skipped: [.[] | select(.has_activity == false) | .repo],
        skipped_count: ([.[] | select(.has_activity == false)] | length)
      }' activity-checks.json > active-repos.json
      
      cat active-repos.json
    output: "active_repos"
    timeout: 60

  # ==========================================================================
  # Step 8: Show discovery summary (Bash for speed and determinism)
  # ==========================================================================
  # BASH STEP - Format and display discovery results
  # Why bash: Structured data formatting is deterministic, faster than LLM
  - id: "discovery-summary"
    type: "bash"
    command: |
      cat << EOF
      ============================================================
      AMPLIFIER ECOSYSTEM ACTIVITY REPORT
      ============================================================
      User:       {{user_info.username}} ({{user_info.filter_mode}})
      Date Range: {{parsed_date.description}}
      
      Discovery:  {{discovered_repos.total}} repos found → {{filtered_repos.count}} after org filter → {{active_repos.count}} with activity
      
      EOF
      
      if [ "{{active_repos.count}}" -gt 0 ]; then
        echo "Analyzing ({{active_repos.count}} repos):"
        echo '{{active_repos.repos}}' | jq -r '.[] | "  • \(.name) - \(.commits) commits, \(.prs) PRs"'
      else
        echo "No repos with activity in date range."
      fi
      
      echo ""
      if [ "{{active_repos.skipped_count}}" -gt 0 ]; then
        echo "Skipped ({{active_repos.skipped_count}} repos - no activity):"
        echo '{{active_repos.skipped}}' | jq -r 'join(", ")' | fold -s -w 70 | sed 's/^/  /'
      fi
      
      echo ""
      echo "Estimated time: ~{{active_repos.count}} minutes (parallel: {{parallel_analysis}})"
      echo "============================================================"
      
      # Output for next step
      echo '{{active_repos.repos}}' | jq -c '{displayed: true, repos_to_analyze: [.[].name]}'
    output: "summary"
    parse_json: true
    timeout: 30

  # ==========================================================================
  # Step 9: Analyze each repo using generic recipe
  # ==========================================================================
  # RECIPE STEP - Delegate detailed analysis to reusable sub-recipe
  # Why recipe: Encapsulates complex per-repo analysis, enables parallel execution
  # Optimization: Pass precomputed values to skip redundant LLM calls in sub-recipe
  - id: "analyze-repos"
    foreach: "{{active_repos.repos}}"
    as: "repo"
    parallel: "{{parallel_analysis}}"
    collect: "repo_analyses"
    type: "recipe"
    recipe: "@recipes:examples/repo-activity-analysis.yaml"
    context:
      repo_url: "{{repo.url}}"
      date_range: "{{parsed_date.description}}"
      working_dir: "{{working_dir}}"
      include_deep_dive: false
      # Precomputed values - sub-recipe skips expensive steps if these are provided
      _precomputed:
        date_since_iso: "{{parsed_date.iso_since}}"
        date_since_date: "{{parsed_date.gh_format}}"
        date_description: "{{parsed_date.description}}"
        repo_owner: "{{repo.owner}}"
        repo_name: "{{repo.name}}"
    output: "single_repo_result"
    timeout: 600
    on_error: "continue"

  # ==========================================================================
  # Step 10: Synthesize final report
  # ==========================================================================
  # AGENT STEP - Synthesize findings into cohesive narrative report
  # Why agent: Requires semantic understanding, prioritization, storytelling
  - id: "synthesize-report"
    agent: "foundation:zen-architect"
    mode: "ARCHITECT"
    prompt: |
      Create the final Amplifier Ecosystem Activity Report.
      
      ## Input Data
      
      User: {{user_info.username}} ({{user_info.filter_mode}})
      Date Range: {{parsed_date.description}}
      Repos analyzed: {{active_repos.count}}
      Repos skipped (no activity): {{active_repos.skipped}}
      
      Individual analyses: {{repo_analyses}}
      
      ## Report Structure
      
      Create comprehensive markdown report:
      
      ```markdown
      # Amplifier Ecosystem Activity Report
      
      **Generated**: [current timestamp]
      **User**: {{user_info.username}}
      **Date Range**: {{parsed_date.description}}
      **Repos Analyzed**: {{active_repos.count}}
      
      ---
      
      ## Executive Summary
      
      - **Total commits**: N
      - **Total PRs**: N  
      - **Repos with activity**: N of {{active_repos.count}}
      
      ### Key Highlights
      
      [2-3 bullet points of most significant changes]
      
      ---
      
      ## Activity by Repository
      
      [For each repo WITH activity, ordered by activity level:]
      
      ### {repo_name}
      
      **Commits**: N | **PRs**: N
      
      **Summary**: [1-2 sentence summary]
      
      **Key Changes**:
      - [Notable changes]
      
      ---
      
      ## Cross-Cutting Observations
      
      [Patterns observed across repos - coordinated changes, themes]
      
      ## Repositories with No Activity
      
      [List repos with no changes in date range]
      
      ---
      *Generated by @amplifier:recipes/ecosystem-activity-report.yaml*
      ```
      
      ## Output
      
      Write report to: {{working_dir}}/reports/{{report_filename}}
      
      Also print the full report to console.
      
      Return the report content.
    output: "final_report"
    timeout: 900
    on_error: "continue"

  # ==========================================================================
  # Step 11: Completion and cleanup
  # ==========================================================================
  # BASH STEP - Print completion summary and remove temp files
  # Why bash: echo/rm are deterministic, no reasoning needed for cleanup
  - id: "complete"
    type: "bash"
    command: |
      # Get report path
      report_path=$(realpath {{working_dir}}/reports/{{report_filename}} 2>/dev/null || echo "{{working_dir}}/reports/{{report_filename}}")
      
      # Print completion summary
      cat << 'COMPLETE_EOF'
      ============================================================
      REPORT COMPLETE
      ============================================================
      COMPLETE_EOF
      echo "User:   {{user_info.username}}"
      echo "Range:  {{parsed_date.description}}"
      echo "Repos:  {{active_repos.count}} analyzed, {{active_repos.skipped_count}} skipped"
      echo "Report: $report_path"
      echo "============================================================"
      
      # Cleanup temporary files (keep reports/)
      echo ""
      echo "Cleaning up temporary files..."
      
      # Remove discovery intermediate files
      rm -rf {{working_dir}}/discovery
      
      # Remove any cloned repos (if any were created)
      rm -rf {{working_dir}}/repos
      
      # Show what remains
      echo "Remaining in {{working_dir}}:"
      ls -la {{working_dir}}/
      
      echo ""
      echo '{"completed": true, "report_path": "'$report_path'", "cleanup": "done"}'
    output: "completion"
    timeout: 60
    on_error: "continue"
